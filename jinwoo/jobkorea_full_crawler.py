#!/usr/bin/env python3
"""
================================================================================
⚠️ 이 소스코드는 100% GitHub Copilot AI에 의해 자동 생성되었습니다.
⚠️ This source code was 100% generated by GitHub Copilot AI.
================================================================================

잡코리아 채용공고 반자동화 크롤링 스크립트
로컬 환경에서 실행하여 420개 전체를 크롤링합니다.

사용법:
1. selenium 설치: pip install selenium webdriver-manager
2. 스크립트 실행: python3 jobkorea_full_crawler.py

Selenium을 사용하여 무료로 크롤링합니다.
"""
import pandas as pd
import json
import re
import time
from pathlib import Path
import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
load_dotenv()

# Selenium 사용
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("[WARN] selenium이 설치되지 않았습니다.")
    print("설치: pip install selenium webdriver-manager")

# Firecrawl 사용 시 필요 (옵션)
try:
    from firecrawl import FirecrawlApp
    FIRECRAWL_AVAILABLE = True
except ImportError:
    FIRECRAWL_AVAILABLE = False

class JobKoreaFullCrawler:
    def __init__(self, api_key=None, use_selenium=True):
        self.api_key = api_key
        self.use_selenium = use_selenium

        # 스크립트 파일 위치 기준으로 data 폴더 경로 계산
        # Jupyter Notebook 환경을 고려하여 __file__ 유무 확인
        try:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(script_dir)
        except NameError:
            # __file__이 없는 경우 (Jupyter Notebook 등)
            # 현재 작업 디렉토리에서 프로젝트 루트 찾기
            current_dir = os.getcwd()
            if 'jinwoo' in current_dir:
                project_root = os.path.dirname(current_dir)
            else:
                project_root = current_dir

        data_dir = os.path.join(project_root, 'data')

        self.progress_file = os.path.join(data_dir, 'crawl_progress.json')
        self.output_file = os.path.join(data_dir, 'jobkorea_crawled_complete_2.csv')
        self.data = []
        
        # Selenium 드라이버 초기화 (우선순위 1)
        self.driver = None
        if self.use_selenium and SELENIUM_AVAILABLE:
            try:
                chrome_options = Options()
                # headless 모드는 일부 환경에서 문제 발생 가능
                # chrome_options.add_argument('--headless')
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                chrome_options.add_argument('--disable-gpu')
                chrome_options.add_argument('--window-size=1920,1080')
                chrome_options.add_argument('--disable-blink-features=AutomationControlled')
                chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
                chrome_options.add_experimental_option('useAutomationExtension', False)
                chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

                self.driver = webdriver.Chrome(
                    service=Service(ChromeDriverManager().install()),
                    options=chrome_options
                )
                print("[OK] Selenium 드라이버 초기화 완료 (무료 크롤링)")
            except Exception as e:
                print(f"[WARN] Selenium 초기화 실패: {e}")
                self.driver = None

        # Firecrawl은 백업용 (우선순위 2)
        self.app = None
        if not self.driver:
            if not self.api_key:
                env_key = os.getenv('FIRECRAWL_API_KEY') or os.getenv('FIRECRAWL_APIKEY')
                if env_key:
                    self.api_key = env_key

            if FIRECRAWL_AVAILABLE and self.api_key:
                try:
                    self.app = FirecrawlApp(api_key=self.api_key)
                    print("[OK] Firecrawl API 초기화 완료 (백업)")
                except Exception as e:
                    print(f"[WARN] Firecrawl 초기화 실패: {e}")

        self.load_progress()
    
    def load_progress(self):
        """저장된 진행상황 로드"""
        try:
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            print(f"기존 진행상황 로드: {len(self.data)}개")
        except FileNotFoundError:
            self.data = []
            print("새로운 크롤링 시작")
    
    def save_progress(self):
        """진행상황 저장"""
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
    
    def extract_info(self, markdown, url, orig_title, orig_company):
        """마크다운에서 정보 추출"""
        # 그룹명 제거
        clean_company = orig_company
        for group in ['\n삼성그룹', '\nSK 그룹', '\nKT그룹', '\nCJ그룹', '\nLG그룹', '\n코오롱그룹']:
            clean_company = clean_company.replace(group, '')
        clean_company = clean_company.strip()
        
        info = {
            '채용제목': orig_title,
            '회사': clean_company,
            '근무지': '',
            '경력': '',
            '학력': '',
            '고용형태': '',
            '상세url': url,
            '상세게시글id': '',
            '주요업무': '',
            '자격요건': '',
            '우대사항': '',
            '채용절차': '',
            '복지 및 혜택': '',
            '급여': '',
            '스킬': '',
            '업종': ''
        }
        
        if not markdown:
            return info
        
        # markdown이 dict 형태로 온 경우 처리 (Firecrawl 응답)
        if isinstance(markdown, dict):
            markdown = markdown.get('markdown', markdown.get('content', ''))

        # markdown 문자열 표현이 포함된 경우 처리
        if isinstance(markdown, str) and markdown.startswith("markdown='"):
            # "markdown='..." 형태를 추출
            match = re.match(r"markdown='(.+)'", markdown, re.DOTALL)
            if match:
                markdown = match.group(1)

        try:
            # 게시글 ID
            match = re.search(r'GI_Read/(\d+)', url)
            if match:
                info['상세게시글id'] = match.group(1)
            
            # 제목 - # \[정규직\] 팜피 AI 개발자 형태
            title_match = re.search(r'^#\s*\\?\[([^\]]+)\\?\]\s*(.+)$', markdown, re.MULTILINE)
            if title_match:
                new_title = f"[{title_match.group(1)}] {title_match.group(2)}".strip()
                if new_title and len(new_title) > 3:
                    info['채용제목'] = new_title
            else:
                # 단순 # 제목 형태
                title_match = re.search(r'^#\s*(.+)$', markdown, re.MULTILINE)
                if title_match:
                    new_title = title_match.group(1).strip()
                    new_title = new_title.replace('\\[', '[').replace('\\]', ']')
                    if new_title and len(new_title) > 3:
                        info['채용제목'] = new_title

            # 회사명 - [**팜피㈜**] 형태
            company_match = re.search(r'\[\*\*([^*\]]+)\*\*\]', markdown)
            if company_match:
                comp = company_match.group(1).strip()
                comp = re.sub(r'\s*\n.*그룹.*$', '', comp, flags=re.MULTILINE)
                comp = re.sub(r'\s*그룹\s*$', '', comp)
                if comp and len(comp) > 1:
                    info['회사'] = comp
            
            # 경력 - "경력 경력무관" 또는 "경력\n경력무관"
            if match := re.search(r'경력\s+(\S+)', markdown):
                career = match.group(1).strip()
                # "경력무관", "신입", "3년↑" 같은 패턴
                if career and not career.startswith('학력'):
                    info['경력'] = career

            # 학력 - "학력 학력무관" 또는 "학력\n학력무관"
            if match := re.search(r'학력\s+(\S+)', markdown):
                edu = match.group(1).strip()
                # "학력무관", "대졸↑", "고졸" 같은 패턴
                if edu and not edu.startswith('스킬'):
                    info['학력'] = edu

            # 고용형태 - Selenium: "고용형태 정규직" or Firecrawl: "고용형태\n\n정규직"
            if match := re.search(r'고용형태\s+([^\n]+?)(?:\s+급여|\s+근무|$)', markdown):
                emp_type = match.group(1).strip()
                # 괄호 포함 처리 "정규직 (수습 6개월)"
                if emp_type and len(emp_type) < 100:
                    info['고용형태'] = emp_type

            # 스킬 - 여러 패턴 시도
            스킬_패턴 = [
                r'스킬\s*,?\s*([^\n]+?)(?:\s+우대조건|\s+핵심|로그인|TOP|궁금|기본우대|$)',
                r'스킬\s*[:\s]*([가-힣A-Za-z0-9,.\s/\-&()]+?)(?:\s+우대조건|\s+근무지|로그인|$)',
                r'필수스킬[:\s]*([^\n]+)',
                r'우대스킬[:\s]*([^\n]+)',
            ]

            for pattern in 스킬_패턴:
                if match := re.search(pattern, markdown, re.MULTILINE):
                    skills = match.group(1).strip()
                    # 맨 앞의 컴마와 공백 제거
                    skills = skills.lstrip(',').strip()
                    # 불필요한 텍스트 필터링
                    if skills and 2 < len(skills) < 300:
                        # 제외할 키워드
                        exclude_words = ['로그인', '궁금', 'TOP', '추천공고', '하고 비슷한', '확인해', '기본우대', '장애인', '보훈대상자']
                        if not any(word in skills for word in exclude_words):
                            info['스킬'] = skills
                            break

            # 근무지 - Selenium: "근무지주소 서울 동작구..." or Firecrawl
            if match := re.search(r'근무지(?:주소|역)?\s+(.+?)(?:\s+지도보기|\s+인근지하철|$)', markdown, re.DOTALL):
                location = match.group(1).strip()
                location = re.sub(r'\s+', ' ', location)
                # 괄호 제거 (신대방동, 프레콘빌딩) 등
                location = re.sub(r'\([^)]*\)', '', location)
                location = location.strip()
                if location and len(location) < 200:
                    info['근무지'] = location

            # 급여 - "급여 회사 내규에 따름"
            if match := re.search(r'급여\s+([가-힣\d\s,~\(\)]+?)(?:\s+근무|지도|지원|$)', markdown):
                salary = match.group(1).strip()
                # 불필요한 단어가 포함되면 종료
                stop_words = ['근무지', '근무시간', '지도보기', '지원자격', '로그인']
                for word in stop_words:
                    if word in salary:
                        salary = salary.split(word)[0].strip()
                        break
                if salary and 5 < len(salary) < 50:
                    info['급여'] = salary

            # 업종 - "업종 IT·인터넷" 또는 "산업(업종)\n포털·컨텐츠·커뮤니티" 형태
            업종_패턴 = [
                r'산업\(업종\)\s*([^\n]+?)(?:\s+지도보기|\s+위치|\s+사원수|$)',
                r'업종\s+([^\n]+?)(?:\s+기업규모|\s+사원수|\s+설립|\s+대표|\s+매출|\s+홈페이지|$)',
                r'업종[:\s]*([가-힣·,/\s&\-A-Za-z]+?)(?:\s+기업규모|\s+사원수|\s+설립|\s+대표|\s+매출|\s+홈페이지|$)',
                r'산업분야\s+([^\n]+?)(?:\s+기업규모|\s+사원수|\s+설립|$)',
                r'주업종\s+([^\n]+?)(?:\s+기업규모|\s+사원수|\s+설립|$)',
            ]

            for pattern in 업종_패턴:
                if match := re.search(pattern, markdown, re.MULTILINE):
                    industry = match.group(1).strip()
                    # 불필요한 공백 제거
                    industry = re.sub(r'\s+', ' ', industry)
                    if industry and 2 < len(industry) < 100:
                        info['업종'] = industry
                        break

            # 주요업무, 자격요건, 우대사항 등 - 더 유연하고 포괄적으로 추출
            # 다양한 표현과 약간의 차이도 모두 인식

            # 주요업무 / 담당업무 / 업무내용 / 직무내용 / 직무소개
            업무_패턴 = [
                r'이런\s*업무를?\s*하시게\s*돼요[!]?[:\s]*(.+?)(?=이런\s*경험|자격\s*요건|우대|전형|채용\s*절차|복지|유의사항|꼭\s*읽어|접수|기업정보|$)',
                r'주요\s*업무[:\s]*(.+?)(?=자격\s*요건|필수|우대|전형|채용|복지|접수|기업|$)',
                r'담당\s*업무[:\s]*(.+?)(?=자격|필수|우대|전형|채용|복지|접수|기업|$)',
                r'업무\s*내용[:\s]*(.+?)(?=자격|필수|우대|전형|채용|복지|접수|기업|$)',
                r'직무\s*내용[:\s]*(.+?)(?=자격|필수|우대|전형|채용|복지|접수|기업|$)',
                r'직무\s*소개[:\s]*(.+?)(?=자격|필수|우대|전형|채용|복지|접수|기업|$)',
                r'수행\s*업무[:\s]*(.+?)(?=자격|필수|우대|전형|채용|복지|접수|기업|$)'
            ]

            for pattern in 업무_패턴:
                if match := re.search(pattern, markdown, re.DOTALL | re.IGNORECASE):
                    content = match.group(1).strip()
                    content = re.sub(r'\s+', ' ', content)
                    if 10 < len(content) < 3000:
                        info['주요업무'] = content[:1500]
                        break

            # 자격요건 / 지원자격 / 필수요건 / 필수자격 / 지원요강 / 필수역량
            자격_패턴 = [
                r'이런\s*경험을?\s*가지신\s*분을?\s*찾고\s*있어요[!]?[:\s]*(.+?)(?=이런\s*경험이|우대|전형|채용\s*절차|복지|유의사항|꼭\s*읽어|접수|기업정보|$)',
                r'자격\s*요건[:\s]*(.+?)(?=우대|전형|채용|복지|근무|접수|기업|$)',
                r'지원\s*자격[:\s]*(.+?)(?=우대|전형|채용|복지|접수|기업|$)',
                r'지원\s*요강[:\s]*(.+?)(?=우대|전형|채용|복지|접수|기업|$)',
                r'필수\s*요건[:\s]*(.+?)(?=우대|전형|채용|복지|접수|기업|$)',
                r'필수\s*자격[:\s]*(.+?)(?=우대|전형|채용|복지|접수|기업|$)',
                r'필수\s*역량[:\s]*(.+?)(?=우대|전형|채용|복지|접수|기업|$)',
                r'요구\s*사항[:\s]*(.+?)(?=우대|전형|채용|복지|접수|기업|$)',
                r'응시\s*자격[:\s]*(.+?)(?=우대|전형|채용|복지|접수|기업|$)'
            ]

            for pattern in 자격_패턴:
                if match := re.search(pattern, markdown, re.DOTALL | re.IGNORECASE):
                    content = match.group(1).strip()
                    content = re.sub(r'\s+', ' ', content)
                    # 단순히 "경력 경력무관 학력 학력무관"만 있는 경우 제외
                    if 15 < len(content) < 3000:
                        if not (content.startswith('경력') and len(content) < 100):
                            info['자격요건'] = content[:1500]
                            break

            # 우대사항 / 우대조건 / 우대요건 / 우대역량 / 가산점
            우대_패턴 = [
                r'이런\s*경험이\s*있다면\s*더욱\s*좋아요[!]?[:\s]*(.+?)(?=전형|채용\s*절차|복지|유의사항|꼭\s*읽어|근무|접수|기업정보|$)',
                r'우대\s*사항[:\s]*(.+?)(?=전형|채용|복지|근무|접수|기업|유의|$)',
                r'우대\s*조건[:\s]*(.+?)(?=전형|채용|복지|접수|기업|유의|$)',
                r'우대\s*요건[:\s]*(.+?)(?=전형|채용|복지|접수|기업|유의|$)',
                r'우대\s*역량[:\s]*(.+?)(?=전형|채용|복지|접수|기업|유의|$)',
                r'가산점[:\s]*(.+?)(?=전형|채용|복지|접수|기업|유의|$)',
                r'우대\s*자격[:\s]*(.+?)(?=전형|채용|복지|접수|기업|유의|$)'
            ]

            for pattern in 우대_패턴:
                if match := re.search(pattern, markdown, re.DOTALL | re.IGNORECASE):
                    content = match.group(1).strip()
                    content = re.sub(r'\s+', ' ', content)
                    if 5 < len(content) < 3000:
                        info['우대사항'] = content[:1500]
                        break

            # 채용절차 / 전형절차 / 선발절차 / 전형방법 / 지원절차
            절차_패턴 = [
                r'채용\s*절차[:\s]*(.+?)(?=복지|유의사항|꼭\s*읽어|근무|접수|기업정보|$)',
                r'전형\s*절차[:\s]*(.+?)(?=복지|유의사항|꼭\s*읽어|근무|접수|기업정보|$)',
                r'선발\s*절차[:\s]*(.+?)(?=복지|유의|참고|근무|접수|기업|$)',
                r'전형\s*방법[:\s]*(.+?)(?=복지|유의|참고|근무|접수|기업|$)',
                r'지원\s*절차[:\s]*(.+?)(?=복지|유의|참고|근무|접수|기업|$)',
                r'채용\s*과정[:\s]*(.+?)(?=복지|유의|참고|근무|접수|기업|$)',
                r'전형\s*과정[:\s]*(.+?)(?=복지|유의|참고|근무|접수|기업|$)',
                r'채용\s*프로세스[:\s]*(.+?)(?=복지|유의|참고|근무|접수|기업|$)'
            ]

            for pattern in 절차_패턴:
                if match := re.search(pattern, markdown, re.DOTALL | re.IGNORECASE):
                    content = match.group(1).strip()
                    content = re.sub(r'\s+', ' ', content)
                    if 5 < len(content) < 2000:
                        info['채용절차'] = content[:800]
                        break

            # 복지 및 혜택 / 복리후생 / 근무환경 / 인사제도 / 처우
            복지_패턴 = [
                r'복리후생에\s*대해\s*더\s*알고\s*싶으신가요[?]?[:\s]*(.+?)(?=유의사항|꼭\s*읽어|접수|기업정보|지원방법|$)',
                r'복지\s*(?:및|,)?\s*혜택[:\s]*(.+?)(?=유의사항|꼭\s*읽어|접수|기업|지원방법|$)',
                r'복리후생[:\s]*(.+?)(?=유의사항|꼭\s*읽어|접수|기업|지원방법|$)',
                r'근무\s*환경[:\s]*(.+?)(?=유의사항|꼭\s*읽어|접수|기업|지원방법|$)',
                r'복지\s*제도[:\s]*(.+?)(?=유의|접수|기업|지원방법|$)',
                r'인사\s*제도[:\s]*(.+?)(?=유의|접수|기업|지원방법|$)',
                r'혜택[:\s]*(.+?)(?=유의|접수|기업|지원방법|$)',
                r'처우[:\s]*(.+?)(?=유의|접수|기업|지원방법|$)'
            ]

            for pattern in 복지_패턴:
                if match := re.search(pattern, markdown, re.DOTALL | re.IGNORECASE):
                    content = match.group(1).strip()
                    content = re.sub(r'\s+', ' ', content)
                    if 5 < len(content) < 2000:
                        info['복지 및 혜택'] = content[:800]
                        break

        except Exception as e:
            print(f"파싱 오류: {e}")
            import traceback
            traceback.print_exc()

        return info
    
    def crawl_url(self, url, max_retries=3):
        """URL 크롤링

        우선순위:
        1. Selenium (무료)
        2. Firecrawl (백업, 유료)
        3. requests (최종 폴백)
        """
        # 1. Selenium 시도 (우선)
        if self.driver:
            for attempt in range(max_retries):
                try:
                    self.driver.get(url)

                    # 페이지 로딩 대기
                    try:
                        # 상세 내용이 로드될 때까지 명시적 대기 (최대 10초)
                        wait = WebDriverWait(self.driver, 10)
                        wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
                    except:
                        pass

                    # 페이지 스크롤 (동적 콘텐츠 로드를 위해)
                    try:
                        self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(2)
                        self.driver.execute_script("window.scrollTo(0, 0);")
                        time.sleep(1)
                    except:
                        pass

                    # "상세요강" 탭 클릭 (있는 경우)
                    try:
                        detail_tab = wait.until(EC.element_to_be_clickable((By.XPATH, "//a[contains(text(), '상세요강')]")))
                        detail_tab.click()
                        time.sleep(2)
                    except:
                        pass  # 탭이 없거나 이미 활성화된 경우

                    # iframe이 로드될 때까지 대기
                    time.sleep(3)

                    # iframe 처리 - 상세 내용이 iframe 안에 있을 수 있음
                    detail_text = ""
                    try:
                        # iframe 찾기 (명시적 대기)
                        wait.until(EC.presence_of_element_located((By.TAG_NAME, 'iframe')))
                        iframes = self.driver.find_elements(By.TAG_NAME, 'iframe')

                        print(f"발견된 iframe 개수: {len(iframes)}")

                        for idx, iframe in enumerate(iframes):
                            iframe_title = iframe.get_attribute('title') or ''
                            iframe_src = iframe.get_attribute('src') or ''

                            print(f"  iframe {idx+1}: title='{iframe_title}', src='{iframe_src[:50] if iframe_src else ''}'...")

                            # 상세 모집 요강 iframe 찾기
                            if '상세' in iframe_title or 'GI_Read_Comt_Ifrm' in iframe_src or 'Comt_Ifrm' in iframe_src:
                                print(f"  [OK] 상세 내용 iframe 발견!")
                                # iframe으로 전환
                                self.driver.switch_to.frame(iframe)
                                time.sleep(2)

                                # iframe 내부 HTML 가져오기
                                iframe_html = self.driver.page_source
                                soup = BeautifulSoup(iframe_html, 'html.parser')

                                # 상세 내용 영역 찾기
                                detail_content = soup.find('article', class_='view-content')
                                if not detail_content:
                                    detail_content = soup.find('div', class_='view-content')
                                if not detail_content:
                                    detail_content = soup.find('div', class_='content_sec')
                                if not detail_content:
                                    detail_content = soup.find('div', class_='tempate-detailed-summary-root')
                                if not detail_content:
                                    detail_content = soup.find('article', class_='view-detail')
                                if not detail_content:
                                    detail_content = soup.find('div', class_='dev-wrap-detailContents')
                                if not detail_content:
                                    # iframe body 전체
                                    detail_content = soup.find('body')

                                if detail_content:
                                    # script, style 태그 제거
                                    for tag in detail_content.find_all(['script', 'style']):
                                        tag.decompose()

                                    # 텍스트 추출
                                    detail_text = detail_content.get_text(separator='\n', strip=True)
                                    print(f"  iframe 내용 길이: {len(detail_text)}자")

                                # iframe에서 나오기
                                self.driver.switch_to.default_content()

                                if detail_text and len(detail_text) > 500:
                                    break
                    except Exception as e:
                        print(f"[WARN] iframe 처리 오류: {e}")
                        import traceback
                        traceback.print_exc()
                        # iframe에서 빠져나오기
                        try:
                            self.driver.switch_to.default_content()
                        except:
                            pass

                    # iframe에서 내용을 못 가져왔다면 메인 페이지에서 시도
                    if not detail_text or len(detail_text) < 500:
                        page_html = self.driver.page_source
                        soup = BeautifulSoup(page_html, 'html.parser')

                        # 상세 내용이 있는 영역 찾기
                        detail_content = soup.find('article', class_='view-content')
                        if not detail_content:
                            detail_content = soup.find('div', class_='view-content')
                        if not detail_content:
                            detail_content = soup.find('div', class_='content_sec')
                        if not detail_content:
                            detail_content = soup.find('div', class_='tempate-detailed-summary-root')
                        if not detail_content:
                            detail_content = soup.find('article', class_='view-detail')
                        if not detail_content:
                            detail_content = soup.find('div', class_='dev-wrap-detailContents')

                        if detail_content:
                            for tag in detail_content.find_all(['script', 'style']):
                                tag.decompose()
                            main_detail_text = detail_content.get_text(separator='\n', strip=True)
                            if len(main_detail_text) > len(detail_text):
                                detail_text = main_detail_text

                    # body 전체 텍스트도 가져오기
                    try:
                        page_text = self.driver.find_element(By.TAG_NAME, 'body').text
                    except:
                        page_text = ""

                    # iframe 내용과 메인 페이지 내용을 결합
                    if detail_text and len(detail_text) > 500:
                        # iframe 내용이 충분하면 page_text와 결합
                        final_text = detail_text + "\n\n" + page_text
                    else:
                        # iframe 내용이 없거나 짧으면 page_text만 사용
                        final_text = page_text

                    if final_text and len(final_text) > 100:
                        print(f"[OK] Selenium 크롤링 성공 (길이: {len(final_text)}자)")
                        return final_text
                    else:
                        print(f"[WARN] Selenium: 페이지 내용이 너무 짧음 (길이: {len(final_text)})")

                except Exception as e:
                    print(f"[WARN] Selenium 크롤링 오류 (시도 {attempt+1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        continue
                    break

        # 2. Firecrawl 백업 (Selenium 실패 시)
        if self.app:
            for attempt in range(max_retries):
                try:
                    # scrapeOptions를 사용하여 더 많은 콘텐츠 가져오기
                    scrape_params = {
                        'formats': ['markdown', 'html'],
                        'onlyMainContent': True,
                        'waitFor': 3000,  # 3초 대기 (JavaScript 렌더링)
                        'includeTags': ['article', 'main', 'div.view-content', 'div.content_sec']
                    }

                    # Firecrawl API 버전에 따라 다른 메서드 사용
                    if hasattr(self.app, 'scrape_url'):
                        result = self.app.scrape_url(url, params=scrape_params)
                    elif hasattr(self.app, 'scrape'):
                        result = self.app.scrape(url, params=scrape_params)
                    else:
                        # 기본 호출 시도
                        result = self.app.scrape_url(url)

                    # 결과 처리
                    markdown_text = None
                    if isinstance(result, dict):
                        markdown_text = result.get('markdown', result.get('content', ''))
                        # HTML도 함께 가져왔다면 HTML에서 추출 시도
                        if not markdown_text or len(markdown_text) < 500:
                            html_content = result.get('html', '')
                            if html_content:
                                markdown_text = self._extract_main_text_from_html(html_content)
                    elif hasattr(result, 'markdown'):
                        markdown_text = result.markdown
                    else:
                        markdown_text = str(result)

                    # markdown_text가 문자열 표현인 경우 처리
                    if isinstance(markdown_text, str) and markdown_text.startswith("markdown='"):
                        match = re.match(r"markdown='(.+)'", markdown_text, re.DOTALL)
                        if match:
                            markdown_text = match.group(1)

                    return markdown_text

                except Exception as e:
                    error_msg = str(e)
                    if 'Rate Limit' in error_msg or 'rate limit' in error_msg:
                        if attempt < max_retries - 1:
                            wait_time = 10
                            print(f"[WAIT] Firecrawl Rate Limit 감지. {wait_time}초 대기... ({attempt+1}/{max_retries})")
                            time.sleep(wait_time)
                            continue
                    print(f"[WARN] Firecrawl 크롤링 오류: {e}")
                    break

        # 3. 최종 폴백: requests + BeautifulSoup
        print(f"[폴백] requests로 크롤링 시도: {url}")
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36'
            }
            resp = requests.get(url, headers=headers, timeout=15)
            resp.raise_for_status()
            html = resp.text

            # HTML에서 주요 텍스트 블록 추출
            text = self._extract_main_text_from_html(html)
            return text
        except requests.RequestException as e:
            print(f"[ERROR] 폴백 요청 실패 ({url}): {e}")
            return None

    def _extract_main_text_from_html(self, html: str) -> str:
        """간단한 heuristic으로 HTML에서 가장 긴 텍스트 블록(본문 유사)을 추출하여 반환합니다.

        - script/style 제거
        - article/main, 또는 div들 중에서 텍스트 길이가 가장 긴 요소 선택
        - 텍스트 전처리(공백 정리)
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')
            # remove script/style
            for s in soup(['script', 'style', 'noscript', 'iframe']):
                s.extract()

            candidates = []
            # 우선 main/article 검색
            for tag in soup.find_all(['article', 'main']):
                txt = tag.get_text(separator=' ', strip=True)
                if txt:
                    candidates.append(txt)

            # div와 section도 후보로
            if not candidates:
                for tag in soup.find_all(['div', 'section']):
                    txt = tag.get_text(separator=' ', strip=True)
                    if txt and len(txt) > 200:  # 짧은 텍스트는 제외
                        candidates.append(txt)

            # fallback 전체 body 텍스트
            if not candidates:
                body = soup.body
                if body:
                    txt = body.get_text(separator=' ', strip=True)
                    candidates.append(txt)

            # 가장 긴 텍스트 선택
            best = max(candidates, key=len) if candidates else ''
            # 줄바꿈 정리, 불필요한 공백 제거
            best = re.sub(r'\s+', ' ', best).strip()
            return best
        except Exception as e:
            print(f"HTML 파싱 오류: {e}")
            return ''

    def crawl_all(self, csv_path):
        """전체 크롤링"""
        import os

        # 파일 존재 확인
        if not os.path.exists(csv_path):
            print(f"[ERROR] 오류: CSV 파일을 찾을 수 없습니다.")
            print(f"   경로: {csv_path}")
            print(f"   절대 경로: {os.path.abspath(csv_path)}")
            print(f"   현재 디렉토리: {os.getcwd()}")
            return

        # CSV 읽기
        try:
            df = pd.read_csv(csv_path)
            print(f"[OK] CSV 파일 로드 성공: {len(df)}개 행")
        except Exception as e:
            print(f"[ERROR] CSV 읽기 오류: {e}")
            return

        # 필수 컬럼 확인
        required_columns = ['url', 'title', 'company']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            print(f"[ERROR] 오류: 필수 컬럼이 없습니다: {missing_columns}")
            print(f"   현재 컬럼: {list(df.columns)}")
            return

        print(f"\n총 {len(df)}개 URL")
        print(f"남은: {len(df)}개")
        print("=" * 70)
        
        for idx, row in df.iterrows():
            print(f"\n[{idx+1}/{len(df)}] 크롤링 중...")
            print(f"제목: {row['title'][:50]}...")

            # 크롤링
            markdown = self.crawl_url(row['url'])

            # 정보 추출
            info = self.extract_info(markdown, row['url'], row['title'], row['company'])
            self.data.append(info)

            # 진행상황 저장 (10개마다)
            if len(self.data) % 10 == 0:
                self.save_progress()
                print(f"[OK] 진행상황 저장: {len(self.data)}개")

            # API 제한 고려 (분당 12개 제한 = 5초 간격)
            time.sleep(6)
        
        # 최종 저장
        self.save_progress()
        self.export_csv()  # CSV 파일 생성
        print(f"\n[OK] 크롤링 완료! 총 {len(self.data)}개")
        print(f"[OK] CSV 파일 저장: {self.output_file}")

    def export_csv(self):
        """CSV 내보내기"""
        if not self.data:
            print("저장할 데이터가 없습니다.")
            return
        
        df = pd.DataFrame(self.data)
        
        columns = ['채용제목', '회사', '근무지', '경력', '학력', '고용형태', 
                   '상세url', '상세게시글id', '주요업무', '자격요건', '우대사항', 
                   '채용절차', '복지 및 혜택', '급여', '스킬', '업종']

        existing_cols = [col for col in columns if col in df.columns]
        df = df[existing_cols]
        
        df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
        print(f"[OK] CSV 저장 완료: {self.output_file}")

    def close(self):
        """리소스 정리"""
        if self.driver:
            try:
                self.driver.quit()
                print("[OK] Selenium 드라이버 종료")
            except:
                pass

    def __del__(self):
        """소멸자 - 자동으로 드라이버 종료"""
        self.close()

def main():
    print("=" * 70)
    print("잡코리아 AI 엔지니어 채용공고 크롤러")
    print("=" * 70)
    
    # API 키 입력
    api_key = ""
    
    if not api_key:
        print("\n[WARN] API 키가 없으면 크롤링을 진행할 수 없습니다.")
        print("Firecrawl API 키는 https://firecrawl.dev 에서 발급받을 수 있습니다.")
        return
    
    # 크롤러 초기화
    crawler = JobKoreaFullCrawler(api_key=api_key)
    
    # CSV 파일 경로 - 스크립트 위치 기준으로 계산
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(os.path.dirname(script_dir), 'data')
    csv_path = os.path.join(data_dir, 'jobkorea_crawler_engineer.csv')

    if not os.path.exists(csv_path):
        print(f"[ERROR] 파일을 찾을 수 없습니다: {csv_path}")
        print(f"현재 작업 디렉토리: {os.getcwd()}")
        return

    print(f"[OK] CSV 파일 확인: {csv_path}")

    # 크롤링 시작
    print(f"\n크롤링 시작합니다...")
    crawler.crawl_all(csv_path)
    
    # CSV 내보내기
    crawler.export_csv()
    
    print("\n완료!")

if __name__ == "__main__":
    main()
