{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time, random\n",
    "import os\n",
    "from datetime import datetime\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaraminCrawler:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        # ì„¹ì…˜ë³„ í‚¤ì›Œë“œ ë§¤í•‘\n",
    "        self.section_patterns = {\n",
    "            'ì£¼ìš”ì—…ë¬´': [\n",
    "                r'ğŸ“‹\\s*ì£¼ìš”ì—…ë¬´(.*?)(?=ğŸ“‹|ğŸ |ğŸ|ğŸš€|$)',\n",
    "                r'â– \\s*ì£¼ìš”\\s*ì—…ë¬´(.*?)(?=â– |$)',\n",
    "                r'â– \\s*ë‹´ë‹¹\\s*ì—…ë¬´(.*?)(?=â– |$)',\n",
    "                r'ì£¼ìš”\\s*ì—…ë¬´\\s*[:\\n]+(.*?)(?=ìê²©|ìš°ëŒ€|ê·¼ë¬´|ë³µì§€|ì±„ìš©|$)',\n",
    "            ],\n",
    "            'ìê²©ìš”ê±´': [\n",
    "                r'ğŸ“‹\\s*ìê²©ìš”ê±´(.*?)(?=ğŸ“‹|ğŸ |ğŸ|ğŸš€|$)',\n",
    "                r'â– \\s*ìê²©\\s*ìš”ê±´(.*?)(?=â– |$)',\n",
    "                r'ìê²©\\s*ìš”ê±´\\s*[:\\n]+(.*?)(?=ìš°ëŒ€|ê·¼ë¬´|ë³µì§€|ì±„ìš©|$)',\n",
    "            ],\n",
    "            'ìš°ëŒ€ì‚¬í•­': [\n",
    "                r'ğŸ“‹\\s*ìš°ëŒ€ì‚¬í•­(.*?)(?=ğŸ“‹|ğŸ |ğŸ|ğŸš€|$)',\n",
    "                r'â– \\s*ìš°ëŒ€\\s*ì‚¬í•­(.*?)(?=â– |$)',\n",
    "                r'ìš°ëŒ€\\s*ì‚¬í•­\\s*[:\\n]+(.*?)(?=ê·¼ë¬´|ë³µì§€|ì±„ìš©|$)',\n",
    "            ],\n",
    "            'ì±„ìš©ì ˆì°¨': [\n",
    "                r'ğŸš€\\s*ì±„ìš©ì ˆì°¨(.*?)(?=ğŸ›ï¸|$)',\n",
    "                r'ì „í˜•\\s*ì ˆì°¨\\s*[:\\n]+(.*?)(?=ìœ ì˜|$)',\n",
    "                r'ì±„ìš©\\s*ì ˆì°¨.*?[:\\n]+(.*?)(?=ìœ ì˜|ì œì¶œ|$)',\n",
    "            ],\n",
    "            'ë³µì§€ ë° í˜œíƒ': [\n",
    "                r'ğŸ\\s*ë³µì§€\\s*ë°\\s*í˜œíƒ(.*?)(?=ğŸš€|$)',\n",
    "                r'ë³µì§€\\s*ë°\\s*í˜œíƒ\\s*[:\\n]+(.*?)(?=ì±„ìš©|ê·¼ë¬´|$)',\n",
    "                r'ë³µë¦¬í›„ìƒ(.*?)(?=ì±„ìš©|ê·¼ë¬´|$)',\n",
    "            ],\n",
    "            'ê¸‰ì—¬': [\n",
    "                r'ê¸‰ì—¬\\s*[:ï¼š]\\s*(.+?)(?=\\n|ê·¼ë¬´|$)',\n",
    "                r'ì—°ë´‰\\s*[:ï¼š]\\s*(.+?)(?=\\n|ê·¼ë¬´|$)',\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì •ì œ\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        # ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "        text = re.sub(r'[ğŸ“‹ğŸ ğŸğŸš€ğŸ›ï¸ã†â€¢Â·]', '', text)\n",
    "        # HTML íƒœê·¸ ì œê±°\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        # ì—°ì† ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        # ì—°ì† ê³µë°± ì •ë¦¬\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def extract_bullet_points(self, text):\n",
    "        \"\"\"ë¶ˆë¦¿ í¬ì¸íŠ¸ í•­ëª© ì¶”ì¶œ\"\"\"\n",
    "        # ë‹¤ì–‘í•œ ë¶ˆë¦¿ í¬ì¸íŠ¸ íŒ¨í„´\n",
    "        patterns = [\n",
    "            r'[-âˆ’â€“â€”]\\s*(.+?)(?=[-âˆ’â€“â€”]|\\n\\n|$)',\n",
    "            r'[â€¢âˆ™Â·]\\s*(.+?)(?=[â€¢âˆ™Â·]|\\n\\n|$)',\n",
    "            r'\\d+[.)]\\s*(.+?)(?=\\d+[.)]|\\n\\n|$)',\n",
    "        ]\n",
    "        items = []\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text, re.MULTILINE | re.DOTALL)\n",
    "            for match in matches:\n",
    "                cleaned = self.clean_text(match)\n",
    "                if len(cleaned) > 5:  # ë„ˆë¬´ ì§§ì€ ê²ƒ ì œì™¸\n",
    "                    items.append(cleaned)\n",
    "        return items\n",
    "    \n",
    "    def extract_section(self, markdown_text, section_name):\n",
    "        \"\"\"íŠ¹ì • ì„¹ì…˜ ì¶”ì¶œ\"\"\"\n",
    "        patterns = self.section_patterns.get(section_name, [])\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, markdown_text, re.DOTALL | re.IGNORECASE)\n",
    "            if match:\n",
    "                content = match.group(1).strip()\n",
    "                # ë¶ˆë¦¿ í¬ì¸íŠ¸ ì¶”ì¶œ ì‹œë„\n",
    "                items = self.extract_bullet_points(content)\n",
    "                if items:\n",
    "                    return ', '.join(items)\n",
    "                # ë¶ˆë¦¿ í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì •ì œ\n",
    "                return self.clean_text(content)\n",
    "        return \"\"\n",
    "    \n",
    "    def extract_all_sections(self, markdown_text):\n",
    "        \"\"\"ëª¨ë“  ì„¹ì…˜ ì¶”ì¶œ\"\"\"\n",
    "        result = {}\n",
    "        for section_name in ['ì£¼ìš”ì—…ë¬´', 'ìê²©ìš”ê±´', 'ìš°ëŒ€ì‚¬í•­', 'ì±„ìš©ì ˆì°¨', 'ë³µì§€ ë° í˜œíƒ', 'ê¸‰ì—¬']:\n",
    "            result[section_name] = self.extract_section(markdown_text, section_name)\n",
    "        return result\n",
    "    \n",
    "    def search_jobs(self, keyword=None, **filters):\n",
    "        \"\"\"ì‹¤ì œ api ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©í•œ ê²€ìƒ‰\"\"\"\n",
    "\n",
    "        jobs = []\n",
    "\n",
    "        # ì‹¤ì œ API URL\n",
    "        api_url = \"https://www.saramin.co.kr/zf_user/search/get-recruit-list\"\n",
    "\n",
    "        # API íŒŒë¼ë¯¸í„° (Network íƒ­ì—ì„œ ë°œê²¬í•œ ê²ƒë“¤)\n",
    "        params = {\n",
    "            'searchType': 'search',\n",
    "            'recruitPage': 1,     # í˜ì´ì§€ë„¤ì´ì…˜\n",
    "            'recruitSort': 'relation',   # ì •ë ¬ê¸°ì¤€(ê´€ë ¨ë„ìˆœ)\n",
    "            'recruitPageCount': 40,    # í•œ íì´ì§€ì— ë³´ì´ëŠ” ê°œìˆ˜\n",
    "            'search_optional_item': 'y',\n",
    "            'search_done': 'y',\n",
    "            'panel_count': 'y',\n",
    "            'preview': 'y',\n",
    "            'mainSearch': 'n'\n",
    "        }\n",
    "\n",
    "        # ê²€ìƒ‰ì–´\n",
    "        if keyword:\n",
    "            params['searchword'] = keyword\n",
    "\n",
    "        # ê³ ê¸‰ í•„í„° ì ìš©\n",
    "        #self._apply_filters(params, filters)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(api_url, params=params, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # JSON ì‘ë‹µ íŒŒì‹± : ì‘ë‹µì€ json í˜•íƒœì´ê¸°ë•Œë¬¸ '{\"count\":\"283\",\"innerHTML\":\"<div>...</div>\"}'\n",
    "            json_data = response.json()\n",
    "\n",
    "            # ì „ì²´ ê³µê³  ìˆ˜ ê³„ì‚°\n",
    "            total_count = int(json_data.get('count', '0').replace(',', ''))\n",
    "\n",
    "            # ê³µê³ ê°€ ë§ìœ¼ë©´ 5í˜ì´ì§€ë§Œ í¬ë¡¤ë§ \n",
    "            max_pages = min((total_count + 39) // 40, 1)\n",
    "\n",
    "            print(f\"ì´ {total_count:,}ê°œ ê³µê³  ë°œê²¬! {max_pages}í˜ì´ì§€ í¬ë¡¤ë§ ì˜ˆì •\")\n",
    "            wait = 1.0\n",
    "            for page in range(1, max_pages + 1):\n",
    "                \n",
    "                print(f\"ğŸ“„ {page}/{max_pages} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\")\n",
    "\n",
    "                params['recruitPage'] = page\n",
    "\n",
    "                try: \n",
    "                    # ê° í˜ì´ì§€ë§ˆë‹¤ ìƒˆë¡œ API í˜¸ì¶œ\n",
    "                    response = requests.get(api_url, params=params, headers=self.headers)\n",
    "                    response.raise_for_status()\n",
    "                    json_data = response.json()  # í•´ë‹¹ í˜ì´ì§€ ë°ì´í„°\n",
    "\n",
    "                    if json_data.get('innerHTML'):\n",
    "                        # ì±„ìš©ê³µê³  ì¶”ì¶œ\n",
    "                        soup = BeautifulSoup(json_data['innerHTML'], 'html.parser')\n",
    "                        json_itmes = soup.find_all('div', class_='item_recruit')\n",
    "                        \n",
    "                        if not json_itmes:\n",
    "                            print(f\"í˜ì´ì§€ {page}ì—ì„œ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                            break\n",
    "\n",
    "                        print(f\"ì´ {len(json_itmes)}ê°œ ìˆ˜ì§‘\")\n",
    "\n",
    "                        for item in json_itmes:\n",
    "                            job_data = self.extract_job_info_from_api(item, keyword or 'ì „ì²´')\n",
    "                            if job_data:\n",
    "                                print(job_data[\"rec_idx\"])\n",
    "                                time.sleep(random.uniform(1.5, 4.5))\n",
    "                                self.detail_webdriver_search(job_data)\n",
    "                                jobs.append(job_data)\n",
    "                                \n",
    "                    else:\n",
    "                        print(f\"í˜ì´ì§€ {page}ì—ì„œ ë°ì´í„°ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "                        break\n",
    "\n",
    "                    time.sleep(random.uniform(1.5, 4.5))  # ì„œë²„ ë¶€í•˜ ë° ë¡œëª» ë°©ì§€ ë°©ì§€\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ì´ˆê¸° ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"âœ… '{keyword or 'ì „ì²´'}' ì´ {len(jobs)}ê°œ ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "\n",
    "        df = pd.DataFrame(jobs)\n",
    "        df.to_csv(\n",
    "            f'../data/ì‚¬ëŒì¸_{keyword} ì±„ìš©ê³µê³ .csv'\n",
    "        )\n",
    "        \n",
    "        # return jobs\n",
    "    \n",
    "    def extract_job_info_from_api(self, item, keyword):\n",
    "        \"\"\"APIì—ì„œ ë°›ì€ HTML êµ¬ì¡°ì— ë§ê²Œ ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "        try:\n",
    "            # ê³µê³  ì œëª© ë° ë§í¬\n",
    "            title_elem = item.select_one('div.area_job > h2.job_tit > a')\n",
    "            title = title_elem.get_text(strip=True) if title_elem else print(\"ê³µê³ ëª… ëª»ì°¾ìŒâŒâŒ\")\n",
    "\n",
    "            # ë§í¬ ì²˜ë¦¬ (ìƒëŒ€ê²½ë¡œ â†’ ì ˆëŒ€ê²½ë¡œ ë³€í™˜)\n",
    "            href = title_elem.get('href') if title_elem else \"\"\n",
    "            link = f\"https://www.saramin.co.kr{href}\" if href else \"\"\n",
    "            \n",
    "            # íšŒì‚¬ëª…\n",
    "            company_elem = item.select_one('div.area_corp > strong.corp_name > a')\n",
    "            company = company_elem.get_text(strip=True) if company_elem else print(\"íšŒì‚¬ëª… ì°¾ì§€ ëª»í•¨âŒâŒ\")\n",
    "\n",
    "            # ë§ˆê°ì¼\n",
    "            deadline_elem = item.select_one('div.area_job > div.job_date > span.date')\n",
    "            deadline = deadline_elem.get_text(strip=True) if deadline_elem else print(\"ë§ˆê°ì¼ ì°¾ì§€ ëª»í•¨âŒâŒ\")\n",
    "\n",
    "            # ìœ„ì¹˜, ê²½ë ¥ ì •ë³´\n",
    "            condition_elem = item.select('div.area_job > div.job_condition > span')\n",
    "\n",
    "            # í‚¤ì›Œë“œ1\n",
    "            keyword_elem = item.select('div.area_job > div.job_sector > b > a')\n",
    "            # í‚¤ì›Œë“œ2\n",
    "            keyword_elem2 = item.select('div.area_job > div.job_sector > a')\n",
    "\n",
    "            # ê¸°ë³¸ê°’ ì„¤ì •\n",
    "            location = \"ì§€ì—­ ì—†ìŒ\"  \n",
    "            career = \"ê²½ë ¥ ì—†ìŒ\"\n",
    "            education = \"í•™ë ¥ ì—†ìŒ\"  \n",
    "            work_type = \"ê·¼ë¬´í˜•íƒœ ì—†ìŒ\"\n",
    "            keyword_text = \"í‚¤ì›Œë“œ ì—†ìŒ\"\n",
    "\n",
    "            # ì•ˆì „í•˜ê²Œ ì¸ë±ìŠ¤ í™•ì¸ í›„ ì¶”ì¶œ\n",
    "            if len(condition_elem) > 0:\n",
    "                location_elem = condition_elem[0].select('a')\n",
    "                location_list = [loc.get_text(strip=True) for loc in location_elem]\n",
    "    \n",
    "                if len(location_list) >= 2:\n",
    "                    # \"ì„œìš¸ ê°•ë‚¨êµ¬\" í˜•íƒœë¡œ\n",
    "                    location = \" \".join(location_list)\n",
    "                elif len(location_list) == 1:\n",
    "                    # \"ì„œìš¸\" ê°™ì´ í•˜ë‚˜ë§Œ ìˆëŠ” ê²½ìš°\n",
    "                    location = location_list[0]\n",
    "                else:\n",
    "                    location = \"ì§€ì—­ ì—†ìŒ\"\n",
    "\n",
    "            # ê²½ë ¥ \n",
    "            if len(condition_elem) > 1:\n",
    "                career_elem = condition_elem[1]\n",
    "                career = career_elem.get_text(strip=True)\n",
    "\n",
    "            # í•™ë ¥\n",
    "            if len(condition_elem) > 2:\n",
    "                edu_elem = condition_elem[2]\n",
    "                education = edu_elem.get_text(strip=True)\n",
    "\n",
    "            # ê·¼ë¬´ ì¡°ê±´\n",
    "            if len(condition_elem) > 3:\n",
    "                work_type_elem = condition_elem[3]\n",
    "                work_type = work_type_elem.get_text(strip=True)\n",
    "\n",
    "            # ê³µê³  ID ì¶”ì¶œ\n",
    "            rec_idx = item.get('value', '')\n",
    "\n",
    "            if len(keyword_elem) > 1:\n",
    "\n",
    "                keyword_list = [loc.get_text(strip=True) for loc in keyword_elem]\n",
    "                if len(keyword_elem) == 1:\n",
    "                    keyword_text = keyword_list[0]\n",
    "                else:\n",
    "                    keyword_text = \" \".join(keyword_list)\n",
    "                \n",
    "            if len(keyword_elem2) > 1:\n",
    "                keyword_list2 = [loc.get_text(strip=True) for loc in keyword_elem2]\n",
    "                if len(keyword_elem) > 1:\n",
    "                    keyword_text += \" \" + \" \".join(keyword_list2)\n",
    "                else:\n",
    "                    if len(keyword_elem2) == 1:\n",
    "                        keyword_text = keyword_list2[0]\n",
    "                    else:\n",
    "                        keyword_text = \" \".join(keyword_list2)\n",
    "\n",
    "            return {\n",
    "                'keyword': keyword,\n",
    "                'title': title,\n",
    "                'company': company,\n",
    "                'location': location,\n",
    "                'career': career,\n",
    "                'education': education,\n",
    "                'work_type': work_type,\n",
    "                'deadline': deadline,\n",
    "                'link': link,\n",
    "                'rec_idx': rec_idx,\n",
    "                'keyword_text': keyword_text,\n",
    "                'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ê³µê³  ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ : {e}\")\n",
    "            print(f\"ë¬¸ì œ ê³µê³  HTML êµ¬ì¡°: {item}\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    \n",
    "    def detail_webdriver_search(self, detail_param):\n",
    "        driver = webdriver.Chrome()\n",
    "        try:\n",
    "            wait = WebDriverWait(driver, 20)\n",
    "            link_url = f\"https://m.saramin.co.kr/job-search/view?rec_idx={detail_param['rec_idx']}\"\n",
    "            driver.get(link_url)    \n",
    "\n",
    "            \n",
    "            wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, \"iframe[id^='iframe_recruit_detail_'], iframe[src*='view-frame']\")))\n",
    "            # wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".job-content\")))\n",
    "            job_content = driver.find_element(By.CSS_SELECTOR, \"body\").text\n",
    "\n",
    "            driver.switch_to.default_content()  # ì°¾ì•„ê°€ê¸° ì´ˆê¸°í™”\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".detail_corp\")))\n",
    "\n",
    "            company_content = driver.find_element(By.CSS_SELECTOR, \".detail_corp\")\n",
    "            company_items = company_content.find_elements(By.CSS_SELECTOR, \".detail_item > dd\")[2].text\n",
    "            content_dict = self.extract_all_sections(job_content)\n",
    "\n",
    "            detail_param[\"responsibilities\"] = content_dict[\"ì£¼ìš”ì—…ë¬´\"]\n",
    "            detail_param[\"qualifications\"] = content_dict[\"ìê²©ìš”ê±´\"]\n",
    "            detail_param[\"preferred_quali\"] = content_dict[\"ìš°ëŒ€ì‚¬í•­\"]\n",
    "            detail_param[\"hiring_process\"] = content_dict[\"ì±„ìš©ì ˆì°¨\"]\n",
    "            detail_param[\"benefits\"] = content_dict[\"ë³µì§€ ë° í˜œíƒ\"]\n",
    "            detail_param[\"salary\"] = content_dict[\"ê¸‰ì—¬\"]\n",
    "            detail_param[\"corp_domain\"] = company_items\n",
    "\n",
    "            driver.close()\n",
    "\n",
    "            return(detail_param)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ê³µê³  ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ : {e}\")\n",
    "            print(f\"ìƒì„¸í˜ì´ì§€ id: {detail_param['rec_idx']}\")\n",
    "            return None\n",
    "        except TimeoutException as e:\n",
    "            print(f\"ê³µê³  ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ : {e}\")\n",
    "            print(f\"ìƒì„¸í˜ì´ì§€ id: {detail_param['rec_idx']}\")\n",
    "            return None\n",
    "    \n",
    "\n",
    "self = SaraminCrawler()\n",
    "\n",
    "self.search_jobs(\"AI ì—ì´ì „íŠ¸\")\n",
    "#self.detail_info_job_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "section_patterns = {\n",
    "    'ì£¼ìš”ì—…ë¬´': [\n",
    "        r'ğŸ“‹\\s*ì£¼ìš”ì—…ë¬´(.*?)(?=ğŸ“‹|ğŸ |ğŸ|ğŸš€|$)',\n",
    "        r'â– \\s*ì£¼ìš”\\s*ì—…ë¬´(.*?)(?=â– |$)',\n",
    "        r'â– \\s*ë‹´ë‹¹\\s*ì—…ë¬´(.*?)(?=â– |$)',\n",
    "        r'ì£¼ìš”\\s*ì—…ë¬´\\s*[:\\n]+(.*?)(?=ìê²©|ìš°ëŒ€|ê·¼ë¬´|ë³µì§€|ì±„ìš©|$)',\n",
    "    ],\n",
    "    'ìê²©ìš”ê±´': [\n",
    "        r'ğŸ“‹\\s*ìê²©ìš”ê±´(.*?)(?=ğŸ“‹|ğŸ |ğŸ|ğŸš€|$)',\n",
    "        r'â– \\s*ìê²©\\s*ìš”ê±´(.*?)(?=â– |$)',\n",
    "        r'ìê²©\\s*ìš”ê±´\\s*[:\\n]+(.*?)(?=ìš°ëŒ€|ê·¼ë¬´|ë³µì§€|ì±„ìš©|$)',\n",
    "    ],\n",
    "    'ìš°ëŒ€ì‚¬í•­': [\n",
    "        r'ğŸ“‹\\s*ìš°ëŒ€ì‚¬í•­(.*?)(?=ğŸ“‹|ğŸ |ğŸ|ğŸš€|$)',\n",
    "        r'â– \\s*ìš°ëŒ€\\s*ì‚¬í•­(.*?)(?=â– |$)',\n",
    "        r'ìš°ëŒ€\\s*ì‚¬í•­\\s*[:\\n]+(.*?)(?=ê·¼ë¬´|ë³µì§€|ì±„ìš©|$)',\n",
    "    ],\n",
    "    'ì±„ìš©ì ˆì°¨': [\n",
    "        r'ğŸš€\\s*ì±„ìš©ì ˆì°¨(.*?)(?=ğŸ›ï¸|$)',\n",
    "        r'ì „í˜•\\s*ì ˆì°¨\\s*[:\\n]+(.*?)(?=ìœ ì˜|$)',\n",
    "        r'ì±„ìš©\\s*ì ˆì°¨.*?[:\\n]+(.*?)(?=ìœ ì˜|ì œì¶œ|$)',\n",
    "    ],\n",
    "    'ë³µì§€ ë° í˜œíƒ': [\n",
    "        r'ğŸ\\s*ë³µì§€\\s*ë°\\s*í˜œíƒ(.*?)(?=ğŸš€|$)',\n",
    "        r'ë³µì§€\\s*ë°\\s*í˜œíƒ\\s*[:\\n]+(.*?)(?=ì±„ìš©|ê·¼ë¬´|$)',\n",
    "        r'ë³µë¦¬í›„ìƒ(.*?)(?=ì±„ìš©|ê·¼ë¬´|$)',\n",
    "    ],\n",
    "    'ê¸‰ì—¬': [\n",
    "        r'ê¸‰ì—¬\\s*[:ï¼š]\\s*(.+?)(?=\\n|ê·¼ë¬´|$)',\n",
    "        r'ì—°ë´‰\\s*[:ï¼š]\\s*(.+?)(?=\\n|ê·¼ë¬´|$)',\n",
    "    ]\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì •ì œ\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    text = re.sub(r'[ğŸ“‹ğŸ ğŸğŸš€ğŸ›ï¸ã†â€¢Â·]', '', text)\n",
    "    \n",
    "    # HTML íƒœê·¸ ì œê±°\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # ì—°ì† ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    \n",
    "    # ì—°ì† ê³µë°± ì •ë¦¬\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_bullet_points(text):\n",
    "    \"\"\"ë¶ˆë¦¿ í¬ì¸íŠ¸ í•­ëª© ì¶”ì¶œ\"\"\"\n",
    "    # ë‹¤ì–‘í•œ ë¶ˆë¦¿ í¬ì¸íŠ¸ íŒ¨í„´\n",
    "    patterns = [\n",
    "        r'[-âˆ’â€“â€”]\\s*(.+?)(?=[-âˆ’â€“â€”]|\\n\\n|$)',\n",
    "        r'[â€¢âˆ™Â·]\\s*(.+?)(?=[â€¢âˆ™Â·]|\\n\\n|$)',\n",
    "        r'\\d+[.)]\\s*(.+?)(?=\\d+[.)]|\\n\\n|$)',\n",
    "    ]\n",
    "    \n",
    "    items = []\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text, re.MULTILINE | re.DOTALL)\n",
    "        for match in matches:\n",
    "            cleaned = clean_text(match)\n",
    "            if len(cleaned) > 5:  # ë„ˆë¬´ ì§§ì€ ê²ƒ ì œì™¸\n",
    "                items.append(cleaned)\n",
    "    \n",
    "    return items\n",
    "\n",
    "def extract_section(markdown_text, section_name):\n",
    "    \"\"\"íŠ¹ì • ì„¹ì…˜ ì¶”ì¶œ\"\"\"\n",
    "    patterns = section_patterns.get(section_name, [])\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, markdown_text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            content = match.group(1).strip()\n",
    "            \n",
    "            # ë¶ˆë¦¿ í¬ì¸íŠ¸ ì¶”ì¶œ ì‹œë„\n",
    "            items = extract_bullet_points(content)\n",
    "            if items:\n",
    "                return ', '.join(items)\n",
    "            \n",
    "            # ë¶ˆë¦¿ í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì •ì œ\n",
    "            return clean_text(content)\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def extract_all_sections(markdown_text):\n",
    "    \"\"\"ëª¨ë“  ì„¹ì…˜ ì¶”ì¶œ\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    for section_name in ['ì£¼ìš”ì—…ë¬´', 'ìê²©ìš”ê±´', 'ìš°ëŒ€ì‚¬í•­', 'ì±„ìš©ì ˆì°¨', 'ë³µì§€ ë° í˜œíƒ', 'ê¸‰ì—¬']:\n",
    "        result[section_name] = extract_section(markdown_text, section_name)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba511fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "detail_param = {\n",
    "    \"keyword\":\"ai ì—”ì§€ë‹ˆì–´\",\n",
    "    \"title\":\"[ì£¼ì‹íšŒì‚¬ í•œìš¸ë°˜ë„ì²´]AI ì—”ì§€ë‹ˆì–´ì±„ìš© (ê²½ë ¥)\",\n",
    "    \"company\":\"ì£¼ì‹íšŒì‚¬ í•œìš¸ë°˜ë„ì²´\",\n",
    "    \"location\":\"ê²½ê¸° êµ°í¬ì‹œ\",\n",
    "    \"career\":\"ê²½ë ¥3ë…„â†‘\",\n",
    "    \"education\":\"ì´ˆëŒ€ì¡¸â†‘\",\n",
    "    \"work_type\":\"ì •ê·œì§\",\n",
    "    \"deadline\":\"~ 02\\/25(ìˆ˜)\",\n",
    "    \"link\":\"https:\\/\\/www.saramin.co.kr\\/zf_user\\/jobs\\/relay\\/view?view_type=search&rec_idx=52917838&location=ts&searchword=ai+%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4&searchType=search&paid_fl=n&search_uuid=878295a2-fb7b-46bb-869b-11c2303efdda\",\n",
    "    \"rec_idx\":\"52917838\",\n",
    "    \"keyword_text\":\"AI(ì¸ê³µì§€ëŠ¥) S\\/W Python AI(ì¸ê³µì§€ëŠ¥) DevOps\",\n",
    "    \"crawled_at\":\"2026-01-27 17:09:48\"\n",
    "  }\n",
    "\n",
    "def detail_webdriver_search(detail_param):\n",
    "\n",
    "    data_list = []\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    link_url = f\"https://m.saramin.co.kr/job-search/view?rec_idx=52777191\"\n",
    "    driver.get(link_url)    \n",
    "\n",
    "    wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, \"iframe[id^='iframe_recruit_detail_'], iframe[src*='view-frame']\")))\n",
    "    # wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".job-content\")))\n",
    "    job_content = driver.find_element(By.CSS_SELECTOR, \"body\").text\n",
    "\n",
    "    driver.switch_to.default_content()  # ì°¾ì•„ê°€ê¸° ì´ˆê¸°í™”\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".detail_corp\")))\n",
    "\n",
    "    company_content = driver.find_element(By.CSS_SELECTOR, \".detail_corp\")\n",
    "    company_items = company_content.find_elements(By.CSS_SELECTOR, \".detail_item > dd\")[2].text\n",
    "    content_dict = extract_all_sections(job_content)\n",
    "\n",
    "    data_list.append(job_content)\n",
    "    data_list.append(company_content)\n",
    "    \n",
    "    detail_param[\"responsibilities\"] = content_dict[\"ì£¼ìš”ì—…ë¬´\"]\n",
    "    detail_param[\"qualifications\"] = content_dict[\"ìê²©ìš”ê±´\"]\n",
    "    detail_param[\"preferred_quali\"] = content_dict[\"ìš°ëŒ€ì‚¬í•­\"]\n",
    "    detail_param[\"hiring_process\"] = content_dict[\"ì±„ìš©ì ˆì°¨\"]\n",
    "    detail_param[\"benefits\"] = content_dict[\"ë³µì§€ ë° í˜œíƒ\"]\n",
    "    detail_param[\"salary\"] = content_dict[\"ê¸‰ì—¬\"]\n",
    "    detail_param[\"corp_domain\"] = company_items\n",
    "\n",
    "\n",
    "    return(detail_param)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "detail_webdriver_search(detail_param)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portenup_ai3_project1_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
